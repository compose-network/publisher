package superblock

import (
	"context"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/rs/zerolog"

	"github.com/ssvlabs/rollup-shared-publisher/x/superblock/proofs"
	apicollector "github.com/ssvlabs/rollup-shared-publisher/x/superblock/proofs/collector"
	"github.com/ssvlabs/rollup-shared-publisher/x/superblock/store"
)

type proofPipeline struct {
	cfg       ProofsConfig
	collector apicollector.Service
	prover    proofs.ProverClient
	sbStore   store.SuperblockStore
	log       zerolog.Logger
	pollEvery time.Duration

	publishFn func(context.Context, *store.Superblock, []byte, *proofs.SuperblockAggOutputs) error

	mu   sync.Mutex
	jobs map[string]proofJob
	quit chan struct{}
	once sync.Once
}

type proofJob struct {
	hash      common.Hash
	number    uint64
	proofType string
}

func newProofPipeline(
	cfg ProofsConfig,
	collector apicollector.Service,
	prover proofs.ProverClient,
	sbStore store.SuperblockStore,
	publishFn func(context.Context, *store.Superblock, []byte, *proofs.SuperblockAggOutputs) error,
	log zerolog.Logger,
) *proofPipeline {
	if !cfg.Enabled || collector == nil || prover == nil {
		return nil
	}
	poll := cfg.Prover.PollInterval
	if poll <= 0 {
		poll = 10 * time.Second
	}
	return &proofPipeline{
		cfg:       cfg,
		collector: collector,
		prover:    prover,
		sbStore:   sbStore,
		publishFn: publishFn,
		log:       log.With().Str("component", "proof-pipeline").Logger(),
		pollEvery: poll,
		jobs:      make(map[string]proofJob),
		quit:      make(chan struct{}),
	}
}

func (p *proofPipeline) Start(ctx context.Context) {
	if p == nil {
		return
	}

	p.log.Info().
		Str("proof_type", p.cfg.Prover.ProofType).
		Dur("poll_interval", p.pollEvery).
		Msg("Proof pipeline enabled")

	go p.pollLoop(ctx)
}

func (p *proofPipeline) Stop() {
	if p == nil {
		return
	}

	p.once.Do(func() { close(p.quit) })
}

// HandleSuperblock processes a given superblock by checking and handling proof submissions required for its processing.
// TODO: fix block numbers
//
//nolint:gocyclo // ok
func (p *proofPipeline) HandleSuperblock(ctx context.Context, sb *store.Superblock) error {
	if p == nil {
		return nil
	}

	p.log.Info().
		Uint64("superblock_number", sb.Number).
		Str("superblock_hash", sb.Hash.Hex()).
		Msg("HandleSuperblock called - checking for proofs")

	// TODO: For testing, can bypass missing proofs by creating dummy submissions
	subs, err := p.collector.ListSubmissions(ctx, sb.Hash)
	if err != nil {
		p.log.Warn().Err(err).Uint64("superblock", sb.Number).Msg("No submissions yet for superblock")
		return err
	}

	p.log.Info().
		Uint64("superblock_number", sb.Number).
		Str("superblock_hash", sb.Hash.Hex()).
		Int("submissions_found", len(subs)).
		Msg("Checking submissions for superblock")

	// TODO: Get ALL submissions from collector regardless of superblock hash
	// and then modify their superblock number/hash to match current superblock
	allSubs := p.collector.GetStats()
	totalSubmissions := allSubs["total_submissions"].(int)

	if len(subs) == 0 && totalSubmissions > 0 {
		p.log.Info().
			Uint64("current_superblock", sb.Number).
			Int("total_submissions_in_collector", totalSubmissions).
			Msg("No submissions for current superblock, but collector has submissions - trying to reuse them")

		// TODO: For testing, get submissions from ANY superblock and modify them to match current one
		// This is a hack to test prover integration without proper coordination
		allSuperblocks := allSubs["submissions_by_superblock"].(map[string]int)
		for sbHash := range allSuperblocks {
			otherHash := common.HexToHash(sbHash)
			otherSubs, err := p.collector.ListSubmissions(ctx, otherHash)
			if err == nil && len(otherSubs) > 0 {
				p.log.Info().
					Str("reusing_from_superblock", sbHash).
					Int("submissions_count", len(otherSubs)).
					Msg("Reusing submissions from different superblock")

				// Modify the submissions to match current superblock
				for i := range otherSubs {
					otherSubs[i].SuperblockNumber = sb.Number
					otherSubs[i].SuperblockHash = sb.Hash
				}
				subs = otherSubs
				break
			}
		}
	}

	if len(subs) == 0 {
		p.log.Info().Uint64("superblock", sb.Number).Msg("No proof submissions available")
		return nil
	}

	for i, sub := range subs {
		p.log.Info().
			Int("submission_index", i).
			Uint64("submission_superblock_number", sub.SuperblockNumber).
			Str("submission_superblock_hash", sub.SuperblockHash.Hex()).
			Uint32("chain_id", sub.ChainID).
			Msg("Found proof submission")
	}

	required := p.requiredChainIDs(subs)
	ready := p.isReady(required, subs)

	p.log.Info().
		Uint64("superblock", sb.Number).
		Interface("required_chain_ids", required).
		Int("submissions_count", len(subs)).
		Bool("ready_for_prover", ready).
		Bool("require_all_chains", p.cfg.Collector.RequireAllChains).
		Msg("Evaluated proof readiness")

	if !ready {
		missing := p.missingChains(required, subs)
		p.log.Info().
			Uint64("superblock", sb.Number).
			Ints("missing_chains", missing).
			Int("received", len(subs)).
			Interface("required_chain_ids", required).
			Msg("Not ready - waiting for remaining chain proofs")
		_ = p.collector.UpdateStatus(ctx, sb.Hash, func(st *proofs.Status) {
			st.Required = required
			if st.State == "" {
				st.State = proofs.StateCollecting
			}
		})
		return nil
	}

	// Rate limiter: Check if there's already a proof in StateProving
	provingCount, err := p.collector.CountProvingJobs(ctx)
	if err != nil {
		p.log.Error().Err(err).Msg("Failed to check proving job count")
		return fmt.Errorf("check proving jobs: %w", err)
	}

	if provingCount > 0 {
		p.log.Info().
			Uint64("superblock", sb.Number).
			Int("proving_count", provingCount).
			Msg("Rate limited: another proof is currently proving, queuing this one")
		_ = p.collector.UpdateStatus(ctx, sb.Hash, func(st *proofs.Status) {
			st.Required = required
			st.State = proofs.StateQueued
			st.Error = ""
		})
		return nil
	}

	job := p.buildProofJobInput(ctx, sb, subs)

	jobID, err := p.prover.RequestProof(ctx, job)
	if err != nil {
		_ = p.collector.UpdateStatus(ctx, sb.Hash, func(st *proofs.Status) {
			st.State = proofs.StateFailed
			st.Error = err.Error()
		})
		return fmt.Errorf("request proof: %w", err)
	}

	if err := p.collector.UpdateStatus(ctx, sb.Hash, func(st *proofs.Status) {
		st.Required = required
		st.State = proofs.StateProving
		st.JobID = jobID
		st.Error = ""
	}); err != nil {
		p.log.Warn().Err(err).Uint64("superblock", sb.Number).Msg("Failed to update status post dispatch")
	}

	p.mu.Lock()
	p.jobs[jobID] = proofJob{hash: sb.Hash, number: sb.Number, proofType: job.ProofType}
	p.mu.Unlock()

	p.log.Info().Str("job_id", jobID).Uint64("superblock", sb.Number).Msg("Proof job dispatched")
	return nil
}

func (p *proofPipeline) requiredChainIDs(subs []proofs.Submission) []uint32 {
	// TODO: For testing, can temporarily comment out the config check and return any chain IDs from submissions
	// if len(p.cfg.Collector.RequiredChainIDs) > 0 {
	//	 return append([]uint32(nil), p.cfg.Collector.RequiredChainIDs...)
	// }
	seen := make(map[uint32]struct{}, len(subs))
	for _, s := range subs {
		seen[s.ChainID] = struct{}{}
	}
	out := make([]uint32, 0, len(seen))
	for id := range seen {
		out = append(out, id)
	}
	return out
}

func (p *proofPipeline) isReady(required []uint32, subs []proofs.Submission) bool {
	if !p.cfg.Collector.RequireAllChains {
		return len(subs) > 0
	}
	have := make(map[uint32]struct{}, len(subs))
	for _, s := range subs {
		have[s.ChainID] = struct{}{}
	}
	for _, id := range required {
		if _, ok := have[id]; !ok {
			return false
		}
	}
	return true
}

func (p *proofPipeline) buildProofJobInput(
	ctx context.Context,
	sb *store.Superblock,
	subs []proofs.Submission,
) proofs.ProofJobInput {
	// Create rollup state transitions from submissions
	rollupStateTransitions := make([]proofs.RollupStateTransition, 0, len(subs))
	for _, s := range subs {
		l2BlockNumberBytes := make([]byte, 32)
		blockNumber := s.Aggregation.L2BlockNumber
		for i := 0; i < 8; i++ {
			l2BlockNumberBytes[31-i] = byte(blockNumber)
			blockNumber >>= 8
		}

		rollupStateTransitions = append(rollupStateTransitions, proofs.RollupStateTransition{
			RollupConfigHash: bytesToInts(s.Aggregation.RollupConfigHash.Bytes()),
			L2PreRoot:        bytesToInts(s.Aggregation.L2PreRoot.Bytes()),
			L2PostRoot:       bytesToInts(s.Aggregation.L2PostRoot.Bytes()),
			L2BlockNumber:    bytesToInts(l2BlockNumberBytes),
		})
	}

	var previousBatch proofs.SuperblockBatch
	if sb.Number > 0 {
		prev, err := p.sbStore.GetSuperblock(ctx, sb.Number-1)
		if err == nil {
			// TODO: Get actual parent superblock batch hash
			parentHashBytes := make([]byte, 32)
			copy(parentHashBytes, prev.Hash.Bytes())
			parentHashInts := bytesToInts(parentHashBytes)

			if len(parentHashInts) == 0 {
				parentHashInts = make([]int, 32)
			}

			previousBatch = proofs.SuperblockBatch{
				SuperblockNumber:          prev.Number,
				ParentSuperblockBatchHash: parentHashInts,
				// TODO: Get actual rollup state transitions for previous batch
				RollupSt: []proofs.RollupStateTransition{},
			}
		}
	}

	newBatch := proofs.SuperblockBatch{
		SuperblockNumber:          sb.Number,
		ParentSuperblockBatchHash: bytesToInts(sb.ParentHash.Bytes()),
		RollupSt:                  rollupStateTransitions,
	}

	// Create aggregation proofs
	aggProofs := make([]proofs.AggregationProofData, 0, len(subs))
	for _, s := range subs {
		// TODO: revert, now mocking
		// raw := s.Aggregation.ABIEncode()
		raw := rawPublicValues
		proofBytes := make([]byte, len(s.Proof))
		copy(proofBytes, s.Proof)

		// Create mailbox info from chain ID
		mailboxInfo := []proofs.MailboxInfo{
			{
				ChainID:    s.ChainID,
				InboxRoot:  make([]byte, 32), // TODO: Get actual inbox root
				OutboxRoot: make([]byte, 32), // TODO: Get actual outbox root
			},
		}

		// Convert vkey to [8]uint32 format expected by Rust
		// TODO: Parse actual vkey data instead of using defaults
		aggVKey := [8]uint32{
			1267174729, 1284041170, 746450416, 924179179,
			1558739647, 1619913785, 1738485538, 449972493,
		}

		aggProofs = append(aggProofs, proofs.AggregationProofData{
			AggregationOutputs: s.Aggregation,
			RawPublicValues:    raw,
			CompressedProof:    proofBytes,
			AggVKey:            aggVKey,
			MailboxInfo:        mailboxInfo,
		})
	}

	return proofs.ProofJobInput{
		ProofType: p.cfg.Prover.ProofType,
		Input: proofs.SuperblockProverInput{
			PreviousBatch:     previousBatch,
			NewBatch:          newBatch,
			AggregationProofs: aggProofs,
		},
	}
}

// func (p *proofPipeline) collectSuperblocks(
//	ctx context.Context,
//	current *store.Superblock,
// ) []proofs.ProverSuperblock {
//	result := []proofs.ProverSuperblock{convertSuperblock(current)}
//	if current.Number > 0 {
//		prev, err := p.sbStore.GetSuperblock(ctx, current.Number-1)
//		if err == nil {
//			result = append([]proofs.ProverSuperblock{convertSuperblock(prev)}, result...)
//		}
//	}
//
//	return result
// }
//
// func convertSuperblock(sb *store.Superblock) proofs.ProverSuperblock {
//	psb := proofs.ProverSuperblock{
//		Number:     sb.Number,
//		Slot:       sb.Slot,
//		ParentHash: sb.ParentHash.Bytes(),
//		Hash:       sb.Hash.Bytes(),
//		MerkleRoot: sb.MerkleRoot.Bytes(),
//		Timestamp:  uint64(sb.Timestamp.Unix()),
//	}
//
//	for _, blk := range sb.L2Blocks {
//		psb.L2Blocks = append(psb.L2Blocks, proofs.ProverL2Block{
//			Slot:            blk.GetSlot(),
//			ChainID:         append([]byte(nil), blk.GetChainId()...),
//			BlockNumber:     blk.GetBlockNumber(),
//			BlockHash:       append([]byte(nil), blk.GetBlockHash()...),
//			ParentBlockHash: append([]byte(nil), blk.GetParentBlockHash()...),
//			IncludedXTs:     cloneSlices(blk.GetIncludedXts()),
//			Block:           append([]byte(nil), blk.GetBlock()...),
//		})
//	}
//
//	for _, xt := range sb.IncludedXTs {
//		psb.IncludedXTs = append(psb.IncludedXTs, xt.Bytes())
//	}
//
//	if sb.L1TransactionHash != (common.Hash{}) {
//		psb.L1TransactionHash = sb.L1TransactionHash.Bytes()
//	}
//
//	return psb
// }
//
// func cloneSlices(src [][]byte) [][]byte {
//	out := make([][]byte, len(src))
//	for i, b := range src {
//		out[i] = append([]byte(nil), b...)
//	}
//
//	return out
// }

// processQueuedJobs attempts to process jobs that are in StateQueued
func (p *proofPipeline) processQueuedJobs(ctx context.Context) {
	if p == nil {
		return
	}

	// Check if we can process more jobs (should be 0 proving jobs now)
	provingCount, err := p.collector.CountProvingJobs(ctx)
	if err != nil {
		p.log.Error().Err(err).Msg("Failed to check proving job count while processing queue")
		return
	}

	if provingCount > 0 {
		p.log.Debug().Int("proving_count", provingCount).Msg("Still have proving jobs, not processing queue")
		return
	}

	// Get queued jobs
	queuedJobs, err := p.collector.ListQueuedJobs(ctx)
	if err != nil {
		p.log.Error().Err(err).Msg("Failed to list queued jobs")
		return
	}

	if len(queuedJobs) == 0 {
		p.log.Debug().Msg("No queued jobs to process")
		return
	}

	// Sort by superblock number to process in order (oldest first)
	// TODO: Add proper sorting if needed, for now just process the first one
	jobToProcess := queuedJobs[0]
	for _, job := range queuedJobs {
		if job.SuperblockNumber < jobToProcess.SuperblockNumber {
			jobToProcess = job
		}
	}

	p.log.Info().
		Uint64("superblock", jobToProcess.SuperblockNumber).
		Str("superblock_hash", jobToProcess.SuperblockHash.Hex()).
		Int("total_queued", len(queuedJobs)).
		Msg("Processing queued proof job")

	// Get the superblock for this job
	sb, err := p.sbStore.GetSuperblock(ctx, jobToProcess.SuperblockNumber)
	if err != nil {
		p.log.Error().
			Err(err).
			Uint64("superblock", jobToProcess.SuperblockNumber).
			Msg("Failed to load superblock for queued job")
		return
	}

	// Process this superblock (this will go through the normal flow but should now pass the rate limiter)
	if err := p.HandleSuperblock(ctx, sb); err != nil {
		p.log.Error().
			Err(err).
			Uint64("superblock", jobToProcess.SuperblockNumber).
			Msg("Failed to process queued superblock")
	}
}

// bytesToInts converts a byte slice to an int slice
func bytesToInts(src []byte) []int {
	out := make([]int, len(src))
	for i, b := range src {
		out[i] = int(b)
	}
	return out
}

func (p *proofPipeline) pollLoop(ctx context.Context) {
	if p == nil {
		return
	}

	ticker := time.NewTicker(p.pollEvery)
	defer ticker.Stop()

	statsTicker := time.NewTicker(5 * p.pollEvery)
	defer statsTicker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-p.quit:
			return
		case <-ticker.C:
			p.pollOnce(ctx)
		case <-statsTicker.C:
			p.logStats()
		}
	}
}

func (p *proofPipeline) pollOnce(ctx context.Context) {
	p.mu.Lock()
	jobs := make(map[string]proofJob, len(p.jobs))
	for id, job := range p.jobs {
		jobs[id] = job
	}
	p.mu.Unlock()

	for id, job := range jobs {
		status, err := p.prover.GetStatus(ctx, id)
		if err != nil {
			p.log.Warn().Err(err).Str("job_id", id).Msg("Failed to fetch proof status")
			continue
		}
		switch strings.ToLower(status.Status) {
		case "pending", "running", "proving":
			continue
		case "failed":
			_ = p.collector.UpdateStatus(ctx, job.hash, func(st *proofs.Status) {
				st.State = proofs.StateFailed
				st.Error = "prover reported failure"
			})
			p.removeJob(id)

			go p.processQueuedJobs(ctx)
		case "completed":
			p.handleCompleted(ctx, id, job, status)
		default:
			p.log.Warn().Str("job_id", id).Str("status", status.Status).Msg("Unknown proof job status")
		}
	}
}

func (p *proofPipeline) handleCompleted(ctx context.Context, jobID string, job proofJob, status proofs.ProofJobStatus) {
	p.log.Info().
		Str("job_id", jobID).
		Uint64("superblock", job.number).
		Str("proof_type", job.proofType).
		Int("proof_size_bytes", len(status.Proof)).
		Interface("proving_time_ms", status.ProvingTimeMS).
		Interface("cycles", status.Cycles).
		Interface("commitment", status.Commitment).
		Interface("superblock_agg_outputs", status.SuperblockAggOutputs).
		Msg("Proof job finished successfully")

	outputs := status.SuperblockAggOutputs
	proofBytes := status.Proof
	if len(proofBytes) == 0 {
		p.log.Warn().Str("job_id", jobID).Msg("Completed proof job returned empty proof")
		_ = p.collector.UpdateStatus(ctx, job.hash, func(st *proofs.Status) {
			st.State = proofs.StateFailed
			st.Error = "empty proof from prover"
		})
		p.removeJob(jobID)
		return
	}

	sb, err := p.sbStore.GetSuperblock(ctx, job.number)
	if err != nil {
		p.log.Error().Err(err).Uint64("superblock", job.number).Msg("Failed to load superblock for proof completion")
		return
	}
	sb.Proof = append([]byte(nil), proofBytes...)

	if err := p.sbStore.StoreSuperblock(ctx, sb); err != nil {
		p.log.Error().Err(err).Uint64("superblock", job.number).Msg("Failed to persist superblock with proof")
		return
	}

	if p.publishFn != nil {
		if err := p.publishFn(ctx, sb, proofBytes, outputs); err != nil {
			p.log.Error().Err(err).Uint64("superblock", job.number).Msg("Failed to publish superblock with proof")
			_ = p.collector.UpdateStatus(ctx, job.hash, func(st *proofs.Status) {
				st.State = proofs.StateFailed
				st.Error = err.Error()
			})
			return
		}
	}

	_ = p.collector.UpdateStatus(ctx, job.hash, func(st *proofs.Status) {
		st.State = proofs.StateComplete
		st.Error = ""
	})
	p.removeJob(jobID)
	p.log.Info().Str("job_id", jobID).Uint64("superblock", job.number).Msg("Proof job completed and published")

	go p.processQueuedJobs(ctx)
}

func (p *proofPipeline) removeJob(jobID string) {
	p.mu.Lock()
	delete(p.jobs, jobID)
	p.mu.Unlock()
}

func (p *proofPipeline) missingChains(required []uint32, subs []proofs.Submission) []int {
	have := make(map[uint32]struct{}, len(subs))
	for _, s := range subs {
		have[s.ChainID] = struct{}{}
	}
	var out []int
	for _, id := range required {
		if _, ok := have[id]; !ok {
			out = append(out, int(id))
		}
	}
	return out
}

func (p *proofPipeline) logStats() {
	p.mu.Lock()
	queued := len(p.jobs)
	p.mu.Unlock()

	if queued == 0 {
		p.log.Debug().Msg("Proof pipeline idle")
		return
	}

	p.log.Info().
		Int("outstanding_jobs", queued).
		Msg("Active proof jobs awaiting completion")
}
